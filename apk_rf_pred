import streamlit as st
import pandas as pd
import numpy as np
import requests
from io import StringIO
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from requests.adapters import HTTPAdapter, Retry
import joblib
import matplotlib.pyplot as plt
import os


# Wska≈∫niki techniczne, SMA, RSI + setup sygna≈Ç√≥w

def sma(series: pd.Series, window: int):
    return series.rolling(window=window, min_periods=1).mean()

def rsi(series: pd.Series, period: int = 14):
    delta = series.diff()
    up = delta.clip(lower=0)
    down = -1 * delta.clip(upper=0)
    ma_up = up.rolling(period, min_periods=1).mean()
    ma_down = down.rolling(period, min_periods=1).mean()
    rs = ma_up / (ma_down.replace(0, np.nan))
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50)

def generate_signals(df: pd.DataFrame, short_window=5, long_window=20, rsi_period=14):
    df = df.copy().reset_index(drop=True)
    df['SMA_short'] = sma(df['Close'], short_window)
    df['SMA_long'] = sma(df['Close'], long_window)
    df['RSI'] = rsi(df['Close'], rsi_period)
    df['prev_short'] = df['SMA_short'].shift(1)
    df['prev_long'] = df['SMA_long'].shift(1)
    signals = []
    for i, row in df.iterrows():
        sig = 'HOLD'
        if i == 0:
            signals.append(sig); continue
        if (row['SMA_short'] > row['SMA_long']) and (df.at[i-1, 'SMA_short'] <= df.at[i-1, 'SMA_long']) and (row['RSI'] < 70 and row['RSI'] > 30):
            sig = 'BUY'
        elif (row['SMA_short'] < row['SMA_long']) and (df.at[i-1, 'SMA_short'] >= df.at[i-1, 'SMA_long']) and (row['RSI'] >= 60 and row['RSI'] <= 90):
            sig = 'SELL'
        signals.append(sig)
    df['Signal'] = signals
    df.drop(['prev_short', 'prev_long'], axis=1, inplace=True)
    return df

# Pobieranie danych - z url stooq.com

@st.cache_data(show_spinner=False)
def pobierz_dane_stooq_cached(ticker: str, start_date: str, end_date: str, retries: int = 3, backoff: float = 0.5):
    """
    Returns DataFrame with Date, Open, High, Low, Close, Volume and filled business days.
    start_date/end_date: 'YYYYMMDD' strings
    """
    session = requests.Session()
    retry_cfg = Retry(total=retries, backoff_factor=backoff, status_forcelist=[429,500,502,503,504])
    session.mount("https://", HTTPAdapter(max_retries=retry_cfg))
    url = f"https://stooq.pl/q/d/l/?s={ticker}&d1={start_date}&d2={end_date}&i=d"
    r = session.get(url, timeout=15)
    if r.status_code != 200 or not r.text.strip():
        return None
    df = pd.read_csv(StringIO(r.text))
    df.rename(columns={
        'Data': 'Date', 'Otwarcie': 'Open', 'Najwyzszy': 'High',
        'Najnizszy': 'Low', 'Zamkniecie': 'Close', 'Wolumen': 'Volume'
    }, inplace=True, errors='ignore')
    df['Date'] = pd.to_datetime(df['Date'])
    df = df[['Date','Open','High','Low','Close','Volume']].dropna().sort_values('Date')
    all_days = pd.date_range(df['Date'].min(), df['Date'].max(), freq='B')
    df = df.set_index('Date').reindex(all_days)
    df.index.name = 'Date'
    df.fillna(method='ffill', inplace=True)
    df.reset_index(inplace=True)
    return df

# Zastosowanie modelu + optymalizacja + prognoza

def prognozuj_random_forest_enhanced(df: pd.DataFrame, days: int, n_lags: int = 5, save_model: bool = True, model_path: str = "best_rf.joblib"):
    df = df[['Date','Open','High','Low','Close','Volume']].copy().reset_index(drop=True)
    # add engineered features: returns, short/long SMA, RSI, volatility
    df['Return'] = df['Close'].pct_change().fillna(0)
    df['Volatility'] = df['Close'].rolling(window=5, min_periods=1).std().fillna(0)
    df['SMA_short'] = sma(df['Close'], min(5, n_lags))
    df['SMA_long'] = sma(df['Close'], 20)
    df['RSI'] = rsi(df['Close'], period=14)

    # build lagged windows (flattened) including engineered features
    feature_cols = ['Open','High','Low','Close','Volume','Return','Volatility','SMA_short','SMA_long','RSI']
    X, y = [], []
    for i in range(n_lags, len(df)):
        X.append(df[feature_cols].iloc[i-n_lags:i].values.flatten())
        y.append(df['Close'].iloc[i])
    X, y = np.array(X), np.array(y)
    if len(X) < 10:
        raise ValueError("Za ma≈Ço danych do trenowania modelu (zwiƒôksz zakres dat).")
    split = int(len(X)*0.8)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    # pipeline with feature selection (SelectFromModel) + scaler + rf
    selector = SelectFromModel(RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1), threshold="median")
    pipe = Pipeline([("scaler", StandardScaler()), ("selector", selector), ("rf", RandomForestRegressor(random_state=42, n_jobs=-1))])

    # use RandomizedSearchCV for broader, faster hyperparameter search in time-series CV
    param_dist = {
        "rf__n_estimators": [100, 200, 300],
        "rf__max_depth": [None, 5, 10, 20],
        "rf__max_features": ["sqrt", "log2", 0.5],
        "rf__min_samples_split": [2, 5, 10],
        "rf__min_samples_leaf": [1, 2, 4]
    }
    tscv = TimeSeriesSplit(n_splits=3)
    rs = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=30, cv=tscv, n_jobs=-1, random_state=42)
    rs.fit(X_train, y_train)
    best = rs.best_estimator_
    y_pred = best.predict(X_test)
    metrics = {
        "R2": r2_score(y_test, y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
        "MAE": mean_absolute_error(y_test, y_pred),
        "BestParams": getattr(rs, 'best_params_', None)
    }
    if save_model:
        try:
            joblib.dump(best, model_path)
        except Exception:
            pass
    # aggregate feature importances per original column across lags
    # Map feature importances back to original feature groups (accounting for selector)
    rf = best.named_steps['rf']
    selector = best.named_steps.get('selector', None)
    rf_importances = rf.feature_importances_
    cols = feature_cols
    n_per_step = len(cols)
    total_features = n_per_step * n_lags
    # Build flattened feature names (for debugging/mapping)
    flat_names = []
    for step in range(n_lags):
        for c in cols:
            flat_names.append(f"{c}_lag{step}")

    # If selector exists, get support mask and expand rf_importances into full-length array
    if selector is not None:
        try:
            support = selector.get_support()
            full_imp = np.zeros(total_features)
            # rf_importances correspond to the selected columns (in order)
            full_imp[support] = rf_importances
        except Exception:
            # fallback: assume rf_importances already refer to full feature set
            full_imp = np.zeros(total_features)
            length = min(len(rf_importances), total_features)
            full_imp[:length] = rf_importances[:length]
    else:
        full_imp = np.zeros(total_features)
        length = min(len(rf_importances), total_features)
        full_imp[:length] = rf_importances[:length]

    agg = {c: 0.0 for c in cols}
    for step in range(n_lags):
        for j, c in enumerate(cols):
            idx = step * n_per_step + j
            if idx < len(full_imp):
                agg[c] += float(full_imp[idx])
    # average across lags
    feat_importances = pd.Series({k: v / n_lags for k, v in agg.items()}).sort_values(ascending=False)
    # iterative forecasting using synthetic rows (use same engineered features as training)
    last = df[feature_cols].iloc[-n_lags:].copy().reset_index(drop=True)
    last_date = df['Date'].iloc[-1]
    preds = []
    for _ in range(days):
        Xp = last.values.flatten().reshape(1, -1)
        pred_close = float(best.predict(Xp)[0])
        next_date = last_date + timedelta(days=1)
        while next_date.weekday() >= 5:
            next_date += timedelta(days=1)
        preds.append({'Date': next_date, 'PredictedClose': pred_close})
        # build engineered-feature row consistent with feature_cols
        prev_close = float(last['Close'].iloc[-1])
        vol_arr = np.append(last['Close'].values, pred_close)
        vol = float(np.std(vol_arr))
        # SMA calculations
        sma_short_w = min(5, n_lags)
        sma_short_val = float(pd.Series(np.append(last['Close'].values, pred_close)).rolling(window=sma_short_w, min_periods=1).mean().iloc[-1])
        sma_long_val = float(pd.Series(np.append(last['Close'].values, pred_close)).rolling(window=20, min_periods=1).mean().iloc[-1])
        # RSI calculation
        rsi_series = rsi(pd.Series(np.append(last['Close'].values, pred_close)), period=14)
        rsi_val = float(rsi_series.iloc[-1])
        ret_val = (pred_close / prev_close - 1.0) if prev_close != 0 else 0.0

        new_row = {
            'Open': prev_close,
            'High': pred_close * 1.01,
            'Low': pred_close * 0.99,
            'Close': pred_close,
            'Volume': float(last['Volume'].iloc[-1]),
            'Return': ret_val,
            'Volatility': vol,
            'SMA_short': sma_short_val,
            'SMA_long': sma_long_val,
            'RSI': rsi_val
        }
        last = pd.concat([last, pd.DataFrame([new_row])], ignore_index=True).iloc[-n_lags:]
        last_date = next_date
    preds_df = pd.DataFrame(preds)
    return preds_df, metrics, feat_importances, best


# UI - interfejs setup

def main():
    st.set_page_config(page_title="Prognozowanie RANDOM FOREST", layout="centered")
    st.title("üìä Random Forest Prognoza")
    col1, col2 = st.columns([2,1])
    with col1:
        ticker = st.text_input("Ticker (np. tsla.us):", value="aapl.us")
        start_date = st.date_input("Data poczƒÖtkowa:", datetime.now().date() - timedelta(days=365*3))
        end_date = st.date_input("Data ko≈Ñcowa:", datetime.now().date())
    with col2:
        days = st.number_input("Dni prognozy (robocze):", min_value=1, max_value=30, value=10)
        n_lags = st.number_input("Liczba dni historycznych (lags):", min_value=3, max_value=20, value=5)
    if st.button("Uruchom (enhanced)"):
        d1 = start_date.strftime("%Y%m%d"); d2 = end_date.strftime("%Y%m%d")
        st.info(f"Pobieram dane dla {ticker}...")
        df = pobierz_dane_stooq_cached(ticker, d1, d2)
        if df is None or df.empty:
            st.error("Brak danych.")
            return
        st.success(f"Pobrano {len(df)} wierszy.")
        try:
            preds_df, metrics, feat_importances, model = prognozuj_random_forest_enhanced(df, int(days), int(n_lags), save_model=True, model_path=f"{ticker.replace('.','_')}_best_rf.joblib")
        except Exception as e:
            st.error(f"B≈ÇƒÖd modelu: {e}"); return
        st.subheader("üìä Metryki modelu")
        c1,c2,c3 = st.columns(3)
        c1.metric("R¬≤", f"{metrics['R2']:.3f}")
        c2.metric("RMSE", f"{metrics['RMSE']:.3f}")
        c3.metric("MAE", f"{metrics['MAE']:.3f}")
        st.markdown("**Najlepsze parametry:**")
        st.json(metrics.get("BestParams", {}))
        st.subheader("üîç Feature importances (avg across lags)")
        st.bar_chart(feat_importances)
        # signals + plotting (reuse original logic)
        hist = df[['Date','Open','High','Low','Close','Volume']].copy()
        synthetic = []
        last_volume = hist['Volume'].iloc[-1] if not hist.empty else 0
        prev_close = hist['Close'].iloc[-1]
        for _, row in preds_df.iterrows():
            synth = {'Date': row['Date'], 'Open': prev_close, 'High': row['PredictedClose']*1.01, 'Low': row['PredictedClose']*0.99, 'Close': row['PredictedClose'], 'Volume': last_volume}
            synthetic.append(synth); prev_close = row['PredictedClose']
        synth_df = pd.DataFrame(synthetic)
        combined = pd.concat([hist, synth_df], ignore_index=True, sort=False).sort_values('Date').reset_index(drop=True)
        combined_ind = generate_signals(combined, short_window=min(5,int(n_lags)), long_window=20, rsi_period=14)
        hist_ind = combined_ind[combined_ind['Date'] <= hist['Date'].max()].reset_index(drop=True)
        preds_ind = combined_ind[combined_ind['Date'] > hist['Date'].max()].reset_index(drop=True)
        # plot
        fig, ax = plt.subplots(figsize=(10,5))
        recent = hist_ind[hist_ind['Date'] >= (hist_ind['Date'].max() - pd.Timedelta(days=30))]
        ax.plot(recent['Date'], recent['Close'], color='steelblue', label='History')
        buys = recent[recent['Signal']=='BUY']; sells = recent[recent['Signal']=='SELL']
        if not buys.empty: ax.scatter(buys['Date'], buys['Close'], marker='^', color='green', s=80)
        if not sells.empty: ax.scatter(sells['Date'], sells['Close'], marker='v', color='red', s=80)
        if not preds_df.empty:
            ax.plot([hist['Date'].max(), preds_df['Date'].iloc[0]], [hist['Close'].iloc[-1], preds_df['PredictedClose'].iloc[0]], color='red', linestyle='--')
            ax.plot(preds_df['Date'], preds_df['PredictedClose'], color='red', marker='o', label='Forecast')
        ax.set_title(ticker.upper()); ax.set_xlabel("Date"); ax.set_ylabel("Close"); ax.grid(alpha=0.3); ax.legend()
        st.pyplot(fig)
        # export functionality
        outname = f"{ticker.replace('.','_')}_prediction_.xlsx"
        with pd.ExcelWriter(outname, engine='openpyxl') as writer:
            hist.to_excel(writer, sheet_name='history', index=False)
            preds_df.to_excel(writer, sheet_name='forecast', index=False)
            hist_ind.to_excel(writer, sheet_name='history_signals', index=False)
            preds_ind.to_excel(writer, sheet_name='forecast_signals', index=False)
        st.download_button("Pobierz Excel", open(outname,"rb"), file_name=outname)

if __name__ == "__main__":
    main()
